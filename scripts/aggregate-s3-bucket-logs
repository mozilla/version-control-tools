#!/usr/bin/env python2
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

# This script is used to aggregate S3 server logs into per-day files.

import datetime
import sys

import boto3
import concurrent.futures as futures


def aggregate_s3_logs(bucket, path):
    today = datetime.datetime.utcnow().date()

    s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucket)

    if not path.endswith('/'):
        path += '/'

    current_day = None
    current_keys = []

    for obj in bucket.objects.filter(Prefix=path):
        name = obj.key
        name = name[len(path):]

        # Default names are of the form YYYY-MM-DD-HH-MM-SS-<RANDOM>.
        # Our aggregate logs have a separate naming scheme. Only process
        # logs belonging to us.
        parts = name.split('-')
        if len(parts) != 7:
            continue

        # A prefix can sneak into the file name. We don't deal with that yet.
        # Assert the year is in the first component.
        year = int(parts[0])
        assert year >= 2014 and year < 2030

        day = datetime.date(int(parts[0]), int(parts[1]), int(parts[2]))

        if day != current_day and current_day is not None and current_keys:
            dest_name = '%s%s' % (path, current_day.isoformat())
            print('aggregating %d keys under %s' % (len(current_keys), dest_name))

            data = []

            def get_key(key):
                return key.get()['Body'].read()

            with futures.ThreadPoolExecutor(32) as e:
                fs = []
                for k in current_keys:
                    fs.append(e.submit(get_key, k))

                for f in fs:
                    data.append(f.result())

            bucket.put_object(Key=dest_name,
                              Body=b''.join(data),
                              ContentType='text/plain')

            with futures.ThreadPoolExecutor(8) as e:
                fs = []
                i = 0
                while True:
                    delete_keys = current_keys[i:i+1000]
                    i += 1000

                    if not delete_keys:
                        break

                    deletes = [{'Key': k.key} for k in delete_keys]
                    fs.append(e.submit(bucket.delete_objects,
                                       Delete={'Objects': deletes}))

                # Will raise if there was an error.
                for f in fs:
                    f.result()

            current_keys = []

        if day >= today:
            print('ignoring %s due to incomplete day' % obj.key)
            continue

        current_day = day
        current_keys.append(obj)


if __name__ == '__main__':
    bucket, path = sys.argv[1:]
    aggregate_s3_logs(bucket, path)
