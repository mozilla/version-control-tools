port={{ kafka_port | default(9092) }}
# Too few network threads could result in requests getting starved and timing
# out.
num.network.threads={{ kafka_network_threads | default(8) }}
num.io.threads={{ kafka_io_threads | default(8) }}
socket.send.buffer.bytes={{ kafka_socket_send_buffer | default(1048576) }}
socket.receive.buffer.bytes={{ kafka_socket_receive_buffer | default(1048576) }}
socket.request.max.bytes={{ kafka_socket_request_max | default(104857600) }}

log.dirs={{ kafka_log_dirs | default("/var/lib/kafka/logs") }}

num.partitions={{ kafka_num_partitions | default(1) }}

# We want to be explicit about what topics are created.
auto.create.topics.enable=false

# Retain logs for 3 days by default.
log.retention.hours={{ kafka_log_retention_hours | default(72) }}
log.segment.bytes={{ kafka_log_segment_bytes | default(536870912) }}
log.retention.check.interval.ms={{ kafka_log_retention_check_interval | default(60000) }}
log.cleaner.enable={{ kafka_log_cleaner_enable | default("false") }}

replica.socket.timeout.ms=5000
replica.lag.time.max.ms=30000

# Default to RF3 and requiring 2 in sync replicas to ensure that
# producers with ack=-1 only succeed if a majority of replicas have
# data.
default.replication.factor={{ kafka_replication_factor | default(3) }}
min.insync.replicas={{ kafka_min_insync_replicas | default(2) }}

# Disable cluster to come back online if data loss may occur.
unclean.leader.election.enable=false

zookeeper.session.timeout.ms={{ kafka_zookeeper_session_timeout | default(3000) }}
zookeeper.connection.timeout.ms={{ kafka_zookeeper_connection_timeout | default(3000) }}

broker.id={{ kafka_broker_id | mandatory }}
advertised.host.name={{ kafka_host_name | mandatory }}
advertised.port={{ kafka_advertised_port | default(9092) }}
zookeeper.connect={{ kafka_zookeeper_connect | mandatory }}

inter.broker.protocol.version=1.1
log.message.format.version=1.1
